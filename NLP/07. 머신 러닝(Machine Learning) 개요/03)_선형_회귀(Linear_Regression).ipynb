{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03) 선형 회귀(Linear Regression).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXDBaoLME09+sWP9pUafss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2pterons/training/blob/main/NLP/07.%20%EB%A8%B8%EC%8B%A0%20%EB%9F%AC%EB%8B%9D(Machine%20Learning)%20%EA%B0%9C%EC%9A%94/03)_%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80(Linear_Regression).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 딥 러닝을 이용한 자연어 처리 입문\n",
        "https://wikidocs.net/21670"
      ],
      "metadata": {
        "id": "Or5ljVncWHSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03) 선형 회귀 (Linear Regression)\n"
      ],
      "metadata": {
        "id": "HqhhHbi71pWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥 러닝을 이해하기 위해서는 선형 회귀(Linear Regression)와 로지스틱 회귀(Logsitic Regression)를 이해할 필요가 있습니다. 이번 챕터에서는 머신 러닝에서 쓰이는 용어인 가설(Hypothesis), 손실 함수(Loss Function) 그리고 경사 하강법(Gradient Descent)에 대한 개념과 선형 회귀에 대해서 이해합니다."
      ],
      "metadata": {
        "id": "8pLi6ZdTWtAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 선형 회귀(Linear Regression)"
      ],
      "metadata": {
        "id": "Xuz5w1922CSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "시험 공부하는 시간을 늘리면 늘릴 수록 성적이 잘 나옵니다. 하루에 걷는 횟수를 늘릴 수록, 몸무게는 줄어듭니다. 집의 평수가 클수록, 집의 매매 가격은 비싼 경향이 있습니다. 이는 수학적으로 생각해보면 어떤 요인의 수치에 따라서 특정 요인의 수치가 영향을 받고있다고 말할 수 있습니다. 조금 더 수학적인 표현을 써보면 어떤 변수의 값에 따라서 특정 변수의 값이 영향을 받고 있다고 불 수 있습니다. 다른 변수의 값을 변하게하는 변수를 $x$, 변수 $x$에 의해서 값이 종속적으로 변하는 변수 $y$라고 해봅시다.\n",
        "<br/>\n",
        "이때 변수 $x$의 값은 독릭적으로 변할수 있는 것에 반해, $y$값은 계속해서 $x$의 값에 의해서, 종석적으로 결정되므로 $x$를 독립 변수, $y$를 종속 변수라고도 합니다. 선형 회귀는 한 개 이상의 독립 변수 $x$와 $y$의 선형 관계를 모델링합니다. 만약, 독립 변수 $x$가 1개라면 단순 선형 회귀라고 합니다."
      ],
      "metadata": {
        "id": "o0zp_riY2FCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 단순 선형 회귀 분석 (Simple Linear Regression Analysis)"
      ],
      "metadata": {
        "id": "Gq4jcyWp2jvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$y = wx + b$$"
      ],
      "metadata": {
        "id": "xdc__zP52jtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 수식은 단순 선형 회귀의 수식을 보여줍니다. 여기서 독립 변수 $x$와 곱해지는 값 $w$를 머신 러닝에서는 가중치(weight), 별도로 더해지는 값 $b$를 편향(bias)이라고 합니다. 직선의 방정식에서는 각각 직선의 기울기와 절편을 의미합니다. $w$와 $b$가 없이 $y$와 $x$란 수식은 $y$는 $x$와 같다는 하나의 식밖에 표현하지 못합니다. 그래프 상으로 말하면 하나의 직선밖에 표현하지 못합니다.\n",
        "$$ y = x $$\n",
        "  \n",
        "다시 말해 $w$와 $b$의 값에 따라서 $x$와 $y$가 표현하는 직선은 무궁무진해집니다."
      ],
      "metadata": {
        "id": "YCreFjSL2gYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 다중 선형 회귀 분석 (Multiple Linear Regression Analysis)"
      ],
      "metadata": {
        "id": "5E_t5SOQ4Nfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ y = w_1x_1 + w_2x_2 + ... w_nx_n + b $$\n",
        "  \n",
        "집의 매매 가격은 단순히 집의 평수가 크다고 결정되는 게 아니라 집의 층의 수, 방의 개수, 지하철 역과의 거리와도 영향이 있습니다. 이러한 다수의 요소를 가지고 집의 매매 가격을 예측해보고 싶습니다. y는 여전히 1개이지만 이제 x는 1개가 아니라 여러개가 되었습니다. 이를 다중 선형 회귀 분석이라고 합니다. 이에 대한 실습은 뒤에서 진행합니다."
      ],
      "metadata": {
        "id": "49y7s1_E319F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ehfj5QmK4qc9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}